name: Benchmarks

on:
  pull_request:
    paths:
      - "Sources/**"
      - "Tests/KSCrashBenchmarks/**"
      - "Tests/KSCrashBenchmarksObjC/**"
      - "Tests/KSCrashBenchmarksCold/**"
      - "Benchmarks/**"
      - "Package.swift"
      - ".github/workflows/benchmarks.yml"

  push:
    branches:
      - master

  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  benchmark:
    runs-on: macos-latest
    # Skip for fork PRs (no write permissions to comment on PR)
    if: |
      github.event_name != 'pull_request' ||
      github.event.pull_request.head.repo.full_name == github.repository

    steps:
      - name: Checkout PR Branch
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
          path: pr-branch

      - name: Checkout Base Branch
        if: github.event_name == 'pull_request'
        uses: actions/checkout@v4
        with:
          ref: ${{ github.base_ref }}
          path: base-branch

      - name: Use Latest Stable Xcode
        uses: maxim-lobanov/setup-xcode@v1
        with:
          xcode-version: latest-stable

      - name: Ensure the Platform is Downloaded
        run: |
          xcodebuild -runFirstLaunch
          xcrun simctl list
          xcodebuild -downloadPlatform iOS
          xcodebuild -runFirstLaunch

      - name: Setup Mise
        uses: jdx/mise-action@v2

      - name: Install xcbeautify
        run: brew install xcbeautify

      - name: Run PR Benchmarks
        working-directory: pr-branch/Benchmarks
        run: |
          mise exec -- tuist generate
          rm -rf ../../pr_benchmarks.xcresult
          set -o pipefail
          xcodebuild -workspace KSCrashBenchmarks.xcworkspace \
            -scheme Benchmarks \
            -destination 'platform=iOS Simulator,name=iPhone 17' \
            -resultBundlePath ../../pr_benchmarks.xcresult \
            test 2>&1 | xcbeautify

      - name: Run Base Benchmarks
        if: github.event_name == 'pull_request'
        run: |
          if [ -d "base-branch/Benchmarks" ] && [ -f "base-branch/Benchmarks/Project.swift" ]; then
            cd base-branch/Benchmarks
            mise exec -- tuist generate
            rm -rf ../../base_benchmarks.xcresult
            set -o pipefail
            xcodebuild -workspace KSCrashBenchmarks.xcworkspace \
              -scheme Benchmarks \
              -destination 'platform=iOS Simulator,name=iPhone 17' \
              -resultBundlePath ../../base_benchmarks.xcresult \
              test 2>&1 | xcbeautify
          fi

      - name: Parse and Format Results
        env:
          IS_PR: ${{ github.event_name == 'pull_request' }}
        run: |
          python3 << 'EOF'
          import json
          import math
          import os
          import statistics
          import subprocess
          from datetime import datetime

          is_pr = os.environ.get('IS_PR', 'false') == 'true'

          # Load test configuration
          with open('pr-branch/.github/data/benchmark-tests.json') as f:
              config = json.load(f)

          categories = {c['id']: c for c in config['categories']}
          tests = config['tests']
          tests_by_name = {t['name']: t for t in tests}

          # Get system info
          try:
              chip = subprocess.check_output(['sysctl', '-n', 'machdep.cpu.brand_string']).decode().strip()
          except:
              chip = subprocess.check_output(['uname', '-m']).decode().strip()

          try:
              mem_bytes = int(subprocess.check_output(['sysctl', '-n', 'hw.memsize']).decode().strip())
              mem_gb = mem_bytes / (1024**3)
          except:
              mem_gb = 0

          try:
              macos_ver = subprocess.check_output(['sw_vers', '-productVersion']).decode().strip()
          except:
              macos_ver = "Unknown"

          try:
              xcode_ver = subprocess.check_output(['xcodebuild', '-version']).decode().split('\n')[0]
          except:
              xcode_ver = "Unknown"

          system_info = f"{chip}, {mem_gb:.0f}GB RAM, macOS {macos_ver}, {xcode_ver}"

          def parse_xcresult(xcresult_path):
              """Parse xcresult bundle and return dict of benchmark results with stats, plus device info"""
              if not os.path.exists(xcresult_path):
                  return None, None

              results = {}
              device_name = None
              try:
                  output = subprocess.check_output([
                      'xcrun', 'xcresulttool', 'get', 'test-results', 'metrics',
                      '--path', xcresult_path
                  ]).decode()
                  data = json.loads(output)

                  for test in data:
                      test_id = test.get('testIdentifier', '')
                      if '/testBenchmark' not in test_id:
                          continue

                      test_name = test_id.split('/testBenchmark')[1].rstrip('()')
                      test_runs = test.get('testRuns', [])
                      if not test_runs:
                          continue

                      if not device_name:
                          device = test_runs[0].get('device', {})
                          device_name = device.get('deviceName')

                      for metric in test_runs[0].get('metrics', []):
                          if metric.get('identifier') == 'com.apple.XCTPerformanceMetric_WallClockTime':
                              measurements = metric.get('measurements', [])
                              if measurements:
                                  results[test_name] = {
                                      'avg': statistics.mean(measurements),
                                      'stddev': statistics.stdev(measurements) if len(measurements) > 1 else 0.0,
                                      'n': len(measurements)
                                  }

              except (subprocess.CalledProcessError, json.JSONDecodeError) as e:
                  print(f"Error parsing xcresult: {e}")
                  return None, None

              return (results if results else None), device_name

          def welch_t_test(mean1, std1, n1, mean2, std2, n2):
              """Perform Welch's t-test and return p-value approximation"""
              if std1 == 0 and std2 == 0:
                  return 1.0 if mean1 == mean2 else 0.0

              se1 = (std1 ** 2) / n1
              se2 = (std2 ** 2) / n2
              se_diff = math.sqrt(se1 + se2)

              if se_diff == 0:
                  return 1.0 if mean1 == mean2 else 0.0

              t_stat = abs(mean1 - mean2) / se_diff

              if se1 + se2 == 0:
                  df = n1 + n2 - 2
              else:
                  df = ((se1 + se2) ** 2) / ((se1 ** 2) / (n1 - 1) + (se2 ** 2) / (n2 - 1))

              if t_stat == 0:
                  return 1.0

              if df > 30:
                  p = 2 * (1 - 0.5 * (1 + math.erf(t_stat / math.sqrt(2))))
              else:
                  p = 2 * (1 - 0.5 * (1 + math.erf(t_stat / math.sqrt(2)))) * (1 + 0.5 / df)

              return max(0, min(1, p))

          # Read benchmark results
          pr_results, device_name = parse_xcresult('pr_benchmarks.xcresult')
          base_results, _ = parse_xcresult('base_benchmarks.xcresult') if is_pr else (None, None)

          def format_time(seconds):
              us = seconds * 1_000_000
              if us < 1:
                  return "<1 Œºs"
              elif us < 1000:
                  return f"{us:.1f} Œºs"
              else:
                  return f"{us/1000:.2f} ms"

          def format_stddev(avg, stddev):
              if avg == 0:
                  return "N/A"
              return f"¬±{(stddev / avg) * 100:.1f}%"

          def get_status(seconds, thresholds):
              if seconds < thresholds['excellent']:
                  return "‚úÖ Excellent"
              elif seconds < thresholds['good']:
                  return "‚úÖ Good"
              elif seconds < thresholds['ok']:
                  return "‚ö†Ô∏è OK"
              else:
                  return "‚ùå Review"

          def get_change(base_stats, pr_stats):
              base_avg, pr_avg = base_stats['avg'], pr_stats['avg']
              if base_avg == 0:
                  return "N/A", 1.0

              pct = ((pr_avg - base_avg) / base_avg) * 100
              p_value = welch_t_test(
                  base_avg, base_stats['stddev'], base_stats['n'],
                  pr_avg, pr_stats['stddev'], pr_stats['n']
              )

              if p_value < 0.05:
                  if pct > 15:
                      return f"‚ö†Ô∏è +{pct:.1f}%", p_value
                  elif pct < -15:
                      return f"‚úÖ {pct:.1f}%", p_value
              return f"{pct:+.1f}%", p_value

          # Build output
          output = ["# üîç KSCrash Performance Benchmarks\n"]
          output.append("*Crash capture performance metrics - lower times are better*\n")
          device_info = f" | **Device:** {device_name}" if device_name else ""
          output.append(f"**Host:** {system_info}{device_info}\n")
          output.append("<details>")
          output.append("<summary><b>üìñ How to interpret results</b></summary>\n")
          output.append("| Std Dev | Interpretation |")
          output.append("|---------|----------------|")
          output.append("| < 5% | Stable, reliable measurement |")
          output.append("| 5-15% | Some variability, typical for most benchmarks |")
          output.append("| > 15% | High variability, interpret with caution |\n")
          output.append("**p-value** (in comparison table): Probability the difference is due to chance. Values < 0.05 indicate statistically significant changes.\n")
          output.append("</details>\n")

          # Comparison section (only for PRs with base results)
          if is_pr and base_results:
              output.append("<details open>")
              output.append("<summary><h2>üìä Changes from Base Branch</h2></summary>\n")
              output.append("*Only showing statistically significant changes (p < 0.05, threshold > 15%)*\n")
              output.append("| Operation | Base | Base œÉ | PR | PR œÉ | Change | p-value |")
              output.append("|-----------|------|--------|-----|------|--------|---------|")

              changes = []
              for test in tests:
                  name = test['name']
                  if name in pr_results and name in base_results:
                      base_stats, pr_stats = base_results[name], pr_results[name]
                      change, p_value = get_change(base_stats, pr_stats)
                      pct = ((pr_stats['avg'] - base_stats['avg']) / base_stats['avg']) * 100 if base_stats['avg'] > 0 else 0
                      p_str = f"{p_value:.4f}" if p_value >= 0.0001 else "<0.0001"
                      row = f"| {test['description']} | {format_time(base_stats['avg'])} | {format_stddev(base_stats['avg'], base_stats['stddev'])} | {format_time(pr_stats['avg'])} | {format_stddev(pr_stats['avg'], pr_stats['stddev'])} | {change} | {p_str} |"
                      changes.append(((p_value, -abs(pct)), row, p_value))

              changes.sort(key=lambda x: x[0])
              shown = 0
              for _, row, p_value in changes:
                  if shown >= 15:
                      break
                  if p_value < 0.05:
                      output.append(row)
                      shown += 1

              if shown == 0:
                  output.append("| *No statistically significant changes detected* | | | | | | |")
              output.append("\n</details>\n")
          elif is_pr and not base_results:
              output.append("<details open>")
              output.append("<summary><h2>üìä Changes from Base Branch</h2></summary>\n")
              output.append("*No benchmarks in base branch - showing PR results only*\n")
              output.append("</details>\n")

          # Generate sections by category
          for cat in config['categories']:
              cat_id = cat['id']
              cat_tests = [t for t in tests if t['category'] == cat_id]
              if not cat_tests:
                  continue

              # Check if any tests in this category have results
              has_results = any(t['name'] in pr_results for t in cat_tests) if pr_results else False
              if not has_results:
                  continue

              output.append("<details open>")
              output.append(f"<summary><h3>{cat['name']}</h3></summary>\n")

              # Build header based on category settings
              if cat.get('perCall'):
                  output.append("| Operation | Time | Std Dev | Per-Call | Status |")
                  output.append("|-----------|------|---------|----------|--------|")
              elif cat.get('hideStdDev'):
                  output.append("| Operation | Time | Status | Notes |")
                  output.append("|-----------|------|--------|-------|")
              else:
                  output.append("| Operation | Time | Std Dev | Status | Notes |")
                  output.append("|-----------|------|---------|--------|-------|")

              for test in cat_tests:
                  name = test['name']
                  if not pr_results or name not in pr_results:
                      continue

                  t = pr_results[name]['avg']
                  stddev = pr_results[name]['stddev']
                  thresholds = cat['thresholds']

                  if cat.get('perCall'):
                      iterations = test.get('iterations', 1)
                      per_call = t / iterations
                      status = get_status(per_call, thresholds)
                      output.append(f"| {test['description']} | {format_time(t)} | {format_stddev(t, stddev)} | {format_time(per_call)} | {status} |")
                  elif cat.get('hideStdDev'):
                      status = get_status(t, thresholds)
                      notes = test.get('notes', '')
                      output.append(f"| {test['description']} | {format_time(t)} | {status} | {notes} |")
                  else:
                      status = get_status(t, thresholds)
                      notes = test.get('notes', '')
                      output.append(f"| {test['description']} | {format_time(t)} | {format_stddev(t, stddev)} | {status} | {notes} |")

              output.append("\n</details>")

          # Summary
          total_tests = len(pr_results) if pr_results else 0
          output.append(f"\n---\n**{total_tests} benchmarks** | _Generated {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}_")

          # Write to file
          with open('benchmark_results.md', 'w') as f:
              f.write('\n'.join(output))

          print('\n'.join(output))
          EOF

      - name: Comment PR with Results
        if: github.event_name == 'pull_request'
        uses: thollander/actions-comment-pull-request@v2
        with:
          filePath: benchmark_results.md
          comment_tag: benchmark-results
